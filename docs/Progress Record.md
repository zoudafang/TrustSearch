## 3.26
### 进展
1.将最初的代码改为面向对象的形式，并完成优化，目前能够在enclave中正常运行  
2.初步制定了测试方法，说明了数据结构、内存占用等情况  
### 计划
1.规范测量方法  
2.重新配置sgx环境（之前一直用的SIM模式，跑不了HW）  
3.进行测试，并对测试结果进行整理分析。
### 想法
1.测试时，测试集是完全随机查询（可能查不到），还是保证一定能查到的前提下的随机？

## 4.2
### 进展
1.在enclave内进行了测试：  
    ①在不同initialize-size下，运行程序能跑的最小HeapMaxSize（Enclave内存占用），图：X : initialize-size大小，Y ： HeapMaxSize大小  
    ②X : initialize-size大小，Y ：1000次查询耗费时间  
    目前观察到的结论为：  
    ①内存占用与index条目个数基本成正比关系  
    ②根据数据结果，当x为125000~135000左右时（即enclave内存占用在90MB左右），曲线斜率有明显改变，与先前经验吻合；但斜率变化前，x与y也存在跳变关系，即x=100000发生跳变，目前原因尚不明确  
2.初步完成PPT的制作
### 计划
1.再挖掘可以insight的方向  
2.对PPT进行修改完善  
### 想法  
1.相似查询的结果条数和查询时间之间的关系：  
    由于相似查询，本质上是进行精确查询，即把所有可能的条目都查找一次，在特征值段的长度和汉明距离确定的情况下，每次相似查询的查询次数流程是完全一致的，所以最后相似个数与查询时间没有关系

## 4.9
### 进展
##### 1.使用hopscotch map
1.图一：当内存占用在90M以下，initialize size一致时，hopscotch map的内存占用小于unordered map，查询时间小一个数量级。但内存占用存在跳变，原因暂时未知。跳变后，随着x的增大，查询时间下降，初步认为原因为：hopscotch map性质的影响大于SGX的EPC换页开销影响；  
2.图二：随着x的增加，y会在一定范围内保持相对稳定，这与map本身性质相符合  
3.图三：x从百万级增长到千万级，而查询时间保持在几秒的范围内。  
##### 2.改变分段数N
1.当N=2、16时，初始条目为10W，查询1000次的时间分别为21s、40s，这个效果过于差就没有测试该情况下的内存占用。  
2.当N=8时，初始条目相等时，内存占用比N=4的情况高，但查询速度提升了几倍。  
N=8时，数据结构的主要变化为：sub index 特征值段->标识符 : uint16_t -- uint32_t *8 ，相对于N=4时，sub index: uint32_t -- uint32_t *4，标识符的冗余增加
### 计划
1.考虑多线程实现，主要为实现query多线程进行  
2.SGXv2使用  
3.学习c++多线程使用方法
### 想法
1.由于在sub-index中查询时，需要线性地依次查询每个index，那么可以先用多线程分别实现单独的sub-index的查询，将串行处理变成并行处理，提高速度。

## 4.23  
### 进展 
1.使用全map(unordered map)存储和map+vector存储；跳变前，后者的查询时间略有增加，但内存使用量减少。时间上的跳变相较于前者有显著的延迟。  
2.使用全map(hopscotch map)存储和map+vector存储；跳变前，两者的查询时间几乎一致，但内存使用量减少。时间上的跳变相较于前者有显著的延迟。  
### 计划  
#### 1.提出三阶段计划：  
阶段一：实现普通的multi index；从sgx版本、分段个数、不同map实现等方面观察系统表现；  
阶段二：使用哈希表和线性表相结合的全新索引结构；哈希表实现对查询的快速响应，线性表实现enclave有限空间造成的影响的延迟；  
阶段三：设计分布式查询机制；multi index的查询逻辑支持分布式设计；  
#### 2.完成PPT的更新  
将最新结果整合到PPT中，按照三个阶段对PPT进行逻辑分节

## 5.8  
### 进展  
#### 1.添加布隆过滤器，减少响应时间  
布隆过滤器：写入1500W数据，占用空间小于30MB  
功能：减少无效查询哈希表的次数，提高查询效率  
流程：初始化时，往sub_index中插入子特征值的同时也往布隆过滤器中插入数据，查询sub_index前，先查询布隆过滤器，存在才查询sub_index  
缺陷：占用一定空间(较小)，插入数据无法删除(可能需要更高级的过滤器，相应的增加空间占用)  
#### 2.线性表作为sub_index：  
采用数组存储sub_index+数组排序后二分查找  
优点：存储空间小，查询效率不低  
缺陷：数据量小时，查询时间不如map；数据量达1500w时，查询时间和map差不多，初始化后添加数据的效率低(用树或skip list？)，另外在1500w数据时，大小达到1.3G，同样超出sgx enclave的大小。存储空间小，查询效率不低，但在插入1500w数据时，大小达到1.3G(超过128M)  
#### 3.增强哈希：  
可以不均匀分段，赋予每段不同的阈值，只需要阈值之和为5，减小查询空间大小。  
### 问题
1.用哈希表+线性表的组合能否有查询速度上的提升？千万级数据下，哈希表+线性表肯定大于1G空间，超出128MB限制；哈希表命中率过低时，大部分数据都要先查哈希表，再查线性表，有两次查询时间，如果哈希表又过大，则可能缺页多次  
2.冷热数据迁移的算法，类似cache和主存间的缓存替换算法吗？LRU，LFU

## 5.14
### 进展  
1.完成Baseline、添加布隆过滤器和hash_map+linear_list的系统示意图以及对两次提升的总结（PPT）  
2.找到上周线性表内存占用偏大的原因：上次full_index使用的vector扩容会直接扩到现有空间的2倍，我们后来观测到1500W的实际空间占用为860MB作用，接近于理论上的750MB  
### 问题  
1.我们在设计hash_map+linear_list的系统图时，正对高低频问题，考虑了两种方式：  
++.给特征值加上 频率属性，方便在初始化和查询时直接判断冷热以及两种表中数据的冷热变换  
++.随机初始化两种表格，通过cache切换，如lru算法等进行动态调整；查询则直接先查询hash map，没查到再查线性表  
2.使用map会存在：**vector占用空间较大**  
sub_index的hash表中(k,v)，v是vector(uint32)用于保存该分段的子特征向量为k的所有向量的标识集合，1个vector对象需要24-30B空间，但vector保存数据较少时(2-3个)，空间利用率较低(数据大小/vector大小)  
**举例**：150W均匀分布的测试数据，每个map中的vector差不多1-3个数据，测试发现hopmap的负载为50%-70%，除去map中的vector，其他空间占用为265M，sub_index的map中vector占用空间445M，大于纯数据的空间占用(24M)  
**影响**：主要查询时间受map的影响较大，vector只有在查询到匹配的子特征向量才会访问到，对查询时间的影响应该不会很大，对空间大小影响较大
### 计划  
1.拿到信工所提供的数据后，完成上述的三代系统的测试，整理结果。
2.继续阅读增强广义鸽洞原理相关论文